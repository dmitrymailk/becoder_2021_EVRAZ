# becoder_2021_EVRAZ
## Задание отборочного этапа на хакатон becoder 2021 на трек Data Science

- Ссылка на ноутбук в colab  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CVFSDknZYTRcNnbNuOw13oA5OL4VSN4L)

# Общая статистика по датасету

- Самая популярная марка Э76ХФ
- Марка Э76ХФ встречается 4953 раза
- Всего пропусков 119806
- Пропусков для самой популярной марки 92965

### Пропуски для интересующих нас колонок

- 'химшлак последний Al2O3' 	= 1689 ~ 34.100545%
- 'химшлак последний CaO ' 	= 521 ~ 10.518877%
- 'химшлак последний R   ' 	= 521 ~ 10.518877%
- 'химшлак последний SiO2' 	= 1689 ~ 34.100545%

### Количество выбросов = 696 

# Топ моделей для определенной колонки
- 'химшлак последний Al2O3' => CatBoostRegressor MSE => 0.21864096794018217
- 'химшлак последний CaO' => TabNet MSE => 2.8724990535003587
- 'химшлак последний R' => TabNet MSE => 0.00614606972300847
- 'химшлак последний SiO2' => TabNet MSE => 0.15857949090211607

Сноска: в некоторых местах TabNet превосходит CatBoostRegressor скорее из-за разницы сидов чем превосходстве архитектур. Также я не пробовал обширное поле гиперпараметров которые предоставляют эти библиотеки, но чаще всего такие докрутки могут позволить выиграть не более чем 0.1 в метрике.

Также некоторые категориальные признаки были отброшены, поскольку я посчитал их совершенно бесполезными, но если на эти фичи посмотрит человек с доменными знаниями он сможет подсказать мне что из них может играть большую роль, а что просто зашумлять модель.

Ниже следует EDA анализ и проверка этих данных на разных моделях. От xgboost заканчивая AutoML.
